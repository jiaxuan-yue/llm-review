# LLM_Review

## 一、Transformer 架构

### （一）Attention 原理

1. 核心流程：输入向量化→线性映射（乘模型参数矩阵）得到 QKV→Q 与 K 转置相乘计算相似度→除以√d_k 缩放（防梯度爆炸）→softmax 归一化得权重→与 V 加权求和生成摘要；多头并行重复该过程，捕获不同子空间依赖。

2. 角色分类：

   * 编码器自注意力：双向覆盖全源句，生成上下文向量；
   * 解码器掩码自注意力：仅关注已生成部分，按目标语言左到右生成；
   * 交叉注意力：解码器通过当前 Query 从源语言 Key-Value 中提取信息，实现翻译 / 摘要对齐。

   

3. 出现原因：

   * 解决长距离依赖：Attention 通过任意两点内积直达，RNN 遍历随距离衰减；
   * 支持并行计算：无先后顺序，整个句子通过一次性矩阵乘法完成处理。

   

### （二）PyTorch 框架

* 定位：承载、训练、优化并运行 Transformer 架构的工程底座，本质是 “矩阵加速器” 和 “自动求导机器”；
* 核心关联：Attention 是 Transformer 的核心算法，PyTorch 为其提供运行支撑。

### （三）Transformer 整体流程

输入 + 位置编码（解决 Attention 对顺序不敏感问题）→多头自注意力→残差连接 & 层归一化（LayerNorm 保证数值稳定）→前馈神经网络（FNN 提供非线性表达能力）→再次残差连接 & 层归一化。

## 二、训练微调

### （一）LoRA 工作原理、优势及适用场景

1. 工作原理：基于低秩近似，ΔW≈BA（秩 r≪min (d,k)）；冻结原权重 W 的梯度，仅对 B、A 反向传播梯度；可训练参数从 d・k 降至 (d+k)・r 个，显存与梯度通信量同步下降；训练过程为前向计算误差 L→反向求解梯度∂L/∂B、∂L/∂A→通过优化器更新 B、A。
2. 优势：数据量更少时不易过拟合，支持可插拔，训练速度快。
3. 适用场景：低资源、多任务 / 多租户场景。
4. 不适用场景：任务差异大、rank 取值小，或需深度修改 token 嵌入、层归一化参数时（LoRA 仅作用于 Attention/MLP 权重）。

### （二）资源有限时选择更大层的策略

1. 核心原则：“广覆盖小 rank” 优于 “单层大 rank”，优先选择权重最大层。

2. 具体策略：

   * 增加覆盖而不提升 rank：将 LoRA 应用于所有线性层（QKVO 等），保持较小 r，效果优于单一层对应大 r；
   * 优先权重更大的层：此类层对模型表达能力贡献最大；
   * rank 选择：通常取 4-64，资源有限时不建议取维度一半，通过 LoRA Config 寻找性能拐点。

   

## 三、提示词工程

### （一）提示词设计

* 核心要点：先进行角色定义（防止幻觉）、约束输出格式、加入动态参数（实现不同分类）。

### （二）Few-shot 模板的原理和使用经验

1. 原理：将 2-3 条静态样例写入 prompt，无需训练，零推理延迟；与 RAG 最终结果注入上下文的效果类似。
2. 辅助技巧：CoT（思维链）引导模型逻辑推理步骤；SoT（重构思维）针对长文本提前预设大纲结构。

### （三）RAG 的工作流程

* 流程：数据切分→向量化→存储→相似检索召回 topk→拼接上下文→LLM 生成。
* 核心区别：与传统模型相比，RAG 靠检索器打分排序（先召回再生成，失败可拒答），传统模型靠自身学习能力（不检索、不更新权重）；二者仅 “拼接上下文放入 prompt” 这一步载体相同，前序流程、成本、效果边界差异显著。

### （四）向量数据库的稀疏与稠密

1. 定义：稠密 = 语义向量；稀疏 = 词频倒排。
2. 作用：混合搜索可实现最高召回率。

## 四、接外部工具

### （一）除 RAG 外访问外部知识 / 执行外部操作的方法

|                      方法                       |                         原理 / 说明                          |
| :---------------------------------------------: | :----------------------------------------------------------: |
| 函数调用与工具使用（Function Calling/Tool Use） | 模型生成符合特定 API 格式的参数（JSON），由后端系统调用外部工具（数据库、搜索 API、计算器等）并返回结果给模型 |
|            智能代理架构（AI Agents）            | 模型可自主决定调用工具的时机、拆解复杂任务，根据结果调整下一步行动 |
|  代码解释器（Code Interpreter/Python Sandbox）  |                通过运行代码实现外部操作与计算                |
|             模型微调（Fine-tuning）             | 用特定领域数据集对预训练模型二次训练（如 LoRA 技术），使模型内化知识，推理时无需依赖外部检索 |
|                 持久化记忆系统                  | 用外部存储（Redis、PostgreSQL、向量库）保存多轮对话摘要或用户画像，下次请求先读取记忆再拼接上下文，实现跨会话个性化 |

### （二）ReAct 调用外部工具的流程

1. 核心范式：Thought（推理）→Action（JSON 行动）→Observation（观察，关键中间反馈）→下一轮 Thought→循环至任务结束。

2. 实现细节：

   * ReAct 是思维范式，可被 LangChain 等框架集成；
   * 流程步骤：开发者通过 Prompt 向大模型（Agent）提供工具定义（功能、参数）→大模型输出特定格式（如 Thought: 、Action: function_call (...)）决定调用的 API→系统执行 API 并将结果以 Observation: 形式注入上下文→驱动模型迭代推理，直至得出最终答案；
   * 关键要素：工具定义、Prompt 引导推理流程、停止标记（Stop Token）。

   

### （三）不同 RAG 变体的特点

|   类型   |                             特点                             |     适用场景     |
| :------: | :----------------------------------------------------------: | :--------------: |
|   RAG    | 纯向量机制，速度最快，流程为 chunk-embedding-ANN（近似最近邻）搜索 - TopK-LLM | 通用快速检索场景 |
| GraphRAG |               基于 “重” 知识图谱做全局社区总结               |   慢速深度问答   |
| LightRAG |           图谱与向量同层索引，轻量快速，边用边更新           |   高频实时场景   |

### （四）外部工具调用的两个核心步骤

1. 工具注册：属于 Agent 的定义与描述阶段，核心是教会模型 “有该工具” 及工具的使用方式；
2. 工具调用：基于 ReAct 等推理策略，模型自主决策并执行工具调用；
3. 设计初衷：克服 “幻觉” 和 “知识滞后”（引入外部权威数据），实现 “解耦” 与 “安全”（模型不直接运行代码）。

## 五、模型性能评估

### （一）大模型性能评估方法与指标

1. 评估维度：

   * 客观技术指标：推断速度、准确率 / 召回率、幻觉率；
   * 通用能力指标：知识储备、推理能力、编程能力、指令遵循度；
   * 主观与竞技场指标：以更强模型（如 GPT-4o）作为 “裁判”；
   * 业务场景特定指标：构建私有评测集。

   

2. 专项评估（RAG 三元组评估）：长文本召回（Needle In A Haystack），测试模型在 200k + 超长上下文中精准提取特定信息点的能力。

### （二）大模型对比评估的人工评估方法

* 核心质量指标：准确真实性、指令遵循度、逻辑连贯性；
* 安全性指标：无害、诚实性。

### （三）安全性控制与准确性保障

1. 安全性控制流程：规则闸门（Neo4j）→判别模型→冲突降级→人工复核→7 年日志留存；

   * 输入侧：强规则闸门，三元组写入 Neo4j 并加唯一索引，毫秒级判断 “是否同方”，命中禁忌直接阻断；
   * 模型侧：轻量级判别模型二次校验；
   * 冲突降级：规则 “禁忌”∩模型 “安全”→相信规则，立即阻断并告警；规则 “安全”∩模型 “禁忌”→相信模型，降级至人工复核；
   * 输出侧：带溯源引用；
   * 人工复核与审计：兜底保障。

   

2. 提高回答准确率的方法：

   * 结构化约束：JSON-Schema 强制要求 + 一次重试（提升至 90%），Few-shot 引导（写入 2-3 条符合要求的示例）；
   * 检索优化：首轮分块过大时进行二次递归分块（召回提升 6%），“query 扩展 + 重排 + 拒答”（召回提升 18%），引用高亮在句末。

   

### （四）长任务报错处理设计

1. 任务拆分：将 “查资料 + 写报告 + 发邮件” 等长任务拆分为独立三步，输出结果落盘并留存日志；
2. 报错处理：自动重试 + 人工兜底；重试策略采用指数退避，即每次失败后，等待时间按 2 的指数倍增长再发起下一次请求。

## 六、知识库检索

### （一）向量检索 “Top-K” 及结果优化

1. 定义：检索过程中召回相关性最高的前 K 个结果。

2. 结果逻辑错误时的调整策略：

   * 首选 Rerank：重新排列搜索结果顺序，确保逻辑通顺的上下文排在前面；
   * 降低温度（Temperature）：让模型生成时更 “保守”，严格参照上下文回答；
   * 优化 Prompt：加入 “如果上下文逻辑不通，请指出” 或 “仅根据提供的参考资料回答” 等约束。

   

### （二）混合检索与重排流程

* 完整链路：Query 改写→知识库混合检索→重排评分→问答；
* 核心环节：检索 / 召回阶段需结合稀疏与稠密向量实现混合搜索，重排阶段需明确评分标准。

## 七、Python 相关

### （一）全局线程锁（GIL）

* 定义：CPython 解释器中的全局互斥锁，同一进程任何时刻仅一个线程执行 Python 字节码；
* 影响：简化内存管理（无需细粒度锁），但导致多线程在 CPU 密集型任务中几乎失效。

### （二）Docker、虚拟机与 Kubernetes（K8s）

|       技术        |       特点       |                       核心操作 / 特性                        |
| :---------------: | :--------------: | :----------------------------------------------------------: |
|      虚拟机       |     独立性强     |                              -                               |
|      Docker       |  过程式，轻量级  | 编写 Dockerfile 定义环境→docker build 构建镜像→docker run 启动容器 |
| Kubernetes（K8s） | 开源容器编排系统 | 声明式 YAML 配置，支持副本管理、自动分配、负载均衡，用于自动化部署、扩展和管理数千个 Docker 容器 |

### （三）FastAPI 框架

1. 定位：Python Web 框架，基于类型提示，支持异步编程。

2. 优势：

   * 高性能（高并发）：基于 ASGI 标准，可处理大量并发请求；
   * 自动生成文档：内置 Swagger UI；
   * 数据验证与安全：依赖 Pydantic 库，自动检查输入数据格式。

   

### （四）函数参数

* *args：接收可变数量的位置参数，类型为元组；
* **kwargs：接收可变数量的关键字参数，类型为字典。

### （五）常用大模型开发框架

|    框架    |             定位             |                           核心能力                           |
| :--------: | :--------------------------: | :----------------------------------------------------------: |
| LangChain  |      大模型应用开发框架      | 抽象 LLM 能力（提示词、模型、记忆、工具）为 “链（Chains）”，实现流程编排 |
| LlamaIndex |  数据与大模型的 “超级桥梁”   |           专注数据连接和 RAG，核心是数据管理和索引           |
|  AutoGen   | 多智能体协作框架（微软开发） | 支持定义多个 Agent，通过彼此 “对话” 解决问题；提供三种调度模式：自动调度、管理器模式、状态机驱动（FSM） |

## 八、其他问题

### （一）IDP 系统核心流程及 Agent 智能体核心组成模块

* （文档未明确给出核心流程及组成模块细节，需结合行业通用认知补充：IDP 系统核心流程通常为文档采集→预处理→识别提取→后处理→输出；Agent 智能体核心组成模块一般包括任务规划器、工具调用器、记忆模块、推理引擎等）

### （二）生成式 AI 优化传统 OCR 识别复杂排版文档的方案

1. 优化方案：结合生成式 AI 技术，对 OCR 识别结果进行纠错、补全（如表格结构还原、手写批注语义解析），利用大模型的上下文理解能力修正排版误差。

2. 优势：提升复杂排版文档（带表格、手写批注）的识别准确率，降低人工校对成本。

3. 潜在风险及应对措施：

   * 风险 1：生成式 AI 产生幻觉导致错误修正；应对：加入溯源机制，绑定 OCR 原始识别结果与 AI 修正结果，支持人工核验；
   * 风险 2：数据隐私泄露；应对：对敏感文档进行脱敏处理，采用私有化部署或合规云服务；
   * 风险 3：性能瓶颈（复杂文档处理耗时）；应对：优化模型推理速度，采用批量处理策略。

   

### （三）规模化多 Agent 协同处理复杂 IDP 任务的架构方案

1. 多 Agent 分工机制：按任务环节拆分 Agent（如文档采集 Agent、预处理 Agent、OCR 优化 Agent、结果校验 Agent），每个 Agent 专注单一职责。
2. 任务调度逻辑：采用中心化调度（如基于 LangChain/AutoGen 框架）或分布式调度，根据任务优先级、Agent 负载动态分配任务，支持任务拆分与合并。
3. 数据流转路径：原始文档→采集 Agent→预处理 Agent（格式标准化）→OCR 识别→OCR 优化 Agent（生成式 AI 修正）→结果校验 Agent（规则 + 人工兜底）→输出结构化数据；数据流转过程中通过向量库 / 数据库存储中间结果，支持溯源与回溯。

### （四）RAG 核心作用

* 解决大模型的知识滞后问题（动态检索外部最新知识）与幻觉问题（基于权威检索结果生成答案）。